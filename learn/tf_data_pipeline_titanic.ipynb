{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow Dataset API\n",
    "This notebook will be focused on understanding the usage og the tensorflow dataset api.\n",
    "\n",
    "First thing we are going to do is  download the sample data from the titanic dataset, pretty standar exercise.\n",
    "Remember the dataset will be downloaded to our local system: ~/.keras/datasets/eval.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ness/.keras/datasets/eval.csv\n"
     ]
    }
   ],
   "source": [
    "TRAIN_DATA_URL = \"https://storage.googleapis.com/tf-datasets/titanic/train.csv\"\n",
    "TEST_DATA_URL = \"https://storage.googleapis.com/tf-datasets/titanic/eval.csv\"\n",
    "train_file_path = tf.keras.utils.get_file(\"train.csv\", TRAIN_DATA_URL)\n",
    "test_file_path = tf.keras.utils.get_file(\"eval.csv\", TEST_DATA_URL)\n",
    "\n",
    "print(test_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization\n",
    "Lets visualize how this file looks like. We are going to use the `head` command. By default it returns the first 10 lines of a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "survived,sex,age,n_siblings_spouses,parch,fare,class,deck,embark_town,alone\n",
      "0,male,22.0,1,0,7.25,Third,unknown,Southampton,n\n",
      "1,female,38.0,1,0,71.2833,First,C,Cherbourg,n\n",
      "1,female,26.0,0,0,7.925,Third,unknown,Southampton,y\n",
      "1,female,35.0,1,0,53.1,First,C,Southampton,n\n",
      "0,male,28.0,0,0,8.4583,Third,unknown,Queenstown,y\n",
      "0,male,2.0,3,1,21.075,Third,unknown,Southampton,n\n",
      "1,female,27.0,0,2,11.1333,Third,unknown,Southampton,n\n",
      "1,female,14.0,1,0,30.0708,Second,unknown,Cherbourg,n\n",
      "1,female,4.0,1,1,16.7,Third,G,Southampton,n\n"
     ]
    }
   ],
   "source": [
    "!head {train_file_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a dataset with tensorflow\n",
    "Next we need to create a dataset we can work with. The `tf.data.experimental.make_csv_dataset` command will create a dataset from the csv file we input. The nice thing is that it will separate our features from our label returning a tuple: (features, label). We need to specify our **target** column in the `label_name` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.experimental.make_csv_dataset(\n",
    "    train_file_path,\n",
    "    batch_size=5,\n",
    "    label_name='survived',\n",
    "    na_value='?',\n",
    "    num_epochs=1,\n",
    "    ignore_errors=True,\n",
    "    shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets analyze the dataset that the function has return. \n",
    "This is the documentation for the `experimental.make_csv_dataset`:\n",
    "Reads CSV files into a dataset, where each element of the dataset is a (features, labels) tuple that corresponds to a batch of CSV rows. The features dictionary maps feature column names to Tensors containing the corresponding feature data, and labels is a Tensor containing the batch's label data\n",
    "\n",
    "**Notice how important is that the features is an ordered dictionary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The label element is a tensor: <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "---> Label Tensor shape:(5,)\n",
      "---> Label content:[0 1 1 1 0]\n",
      "The features element is a dictionary: <class 'collections.OrderedDict'>\n",
      "---> features contains 9 (key, value) objects\n",
      "--->sex                 :[b'male' b'female' b'female' b'female' b'male']\n",
      "--->age                 :[22. 38. 26. 35. 28.]\n",
      "--->n_siblings_spouses  :[1 1 0 1 0]\n",
      "--->parch               :[0 0 0 0 0]\n",
      "--->fare                :[ 7.25   71.2833  7.925  53.1     8.4583]\n",
      "--->class               :[b'Third' b'First' b'Third' b'First' b'Third']\n",
      "--->deck                :[b'unknown' b'C' b'unknown' b'C' b'unknown']\n",
      "--->embark_town         :[b'Southampton' b'Cherbourg' b'Southampton' b'Southampton' b'Queenstown']\n",
      "--->alone               :[b'n' b'n' b'y' b'n' b'y']\n"
     ]
    }
   ],
   "source": [
    "#for element in dataset.take(1):\n",
    " #   print(f'Each element of the dataset is a tuple(features, labels)-> Type: {type(element)}, length:{len(element)}')\n",
    "\n",
    "# Tuple (Features: OrderedDict, label: Tensor)\n",
    "for features, label in dataset.take(1):\n",
    "    print(f'The label element is a tensor: {type(label)}')\n",
    "    print(f'---> Label Tensor shape:{label.shape}')\n",
    "    print(f'---> Label content:{label.numpy()}')\n",
    "    print(f'The features element is a dictionary: {type(features)}')\n",
    "    print(f'---> features contains {len(features)} (key, value) objects')\n",
    "    for (key, value) in features.items():\n",
    "        print(\"--->{:20s}:{}\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![From csv to Dataset](figs/csv_to_dataset.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets put everything into functions, so we can work without repeating code\n",
    "\n",
    "# Create a function that uses make_csv_dataset\n",
    "def create_dataset(file_path, label_name, **kwargs):\n",
    "    dataset = tf.data.experimental.make_csv_dataset(\n",
    "        file_path,\n",
    "        batch_size=5,\n",
    "        label_name=label_name,\n",
    "        na_value='?',\n",
    "        num_epochs=1,\n",
    "        ignore_errors=True,\n",
    "        shuffle=False,\n",
    "        **kwargs)\n",
    "    return dataset\n",
    "\n",
    "# create a function that describes the dataset\n",
    "def describe_dataset(dataset):\n",
    "    for features, label in dataset.take(1):\n",
    "        print(f'The label element is a tensor: {type(label)}')\n",
    "        print(f'---> Label Tensor shape:{label.shape}')\n",
    "        print(f'---> Label content:{label.numpy()}')\n",
    "        print(f'The features element is a dictionary: {type(features)}')\n",
    "        print(f'---> features contains {len(features)} (key, value) objects')\n",
    "        for (key, value) in features.items():\n",
    "            print(\"--->{:20s}:{}\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a number of things we can do with this dataset.\n",
    "Suppose you don't want all the columns of the CSV file, you just need some of them. \n",
    "You can specify what you want in a list array:\n",
    "`SELECT_COLUMNS = ['survived', 'age', 'n_siblings_spouses', 'class', 'deck', 'alone']`\n",
    "And create a new dataset with just that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The label element is a tensor: <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "---> Label Tensor shape:(5,)\n",
      "---> Label content:[0 1 1 1 0]\n",
      "The features element is a dictionary: <class 'collections.OrderedDict'>\n",
      "---> features contains 5 (key, value) objects\n",
      "--->age                 :[22. 38. 26. 35. 28.]\n",
      "--->n_siblings_spouses  :[1 1 0 1 0]\n",
      "--->class               :[b'Third' b'First' b'Third' b'First' b'Third']\n",
      "--->deck                :[b'unknown' b'C' b'unknown' b'C' b'unknown']\n",
      "--->alone               :[b'n' b'n' b'y' b'n' b'y']\n"
     ]
    }
   ],
   "source": [
    "SELECT_COLUMNS = ['survived', 'age', 'n_siblings_spouses', 'class', 'deck', 'alone']\n",
    "temp_dataset = create_dataset(train_file_path, label_name='survived', select_columns=SELECT_COLUMNS)\n",
    "describe_dataset(temp_dataset)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4bd624a0593993fe43ac4046b27b898fb2ef75c21c08f81e89e64ea0f51df676"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('tensorflow': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
