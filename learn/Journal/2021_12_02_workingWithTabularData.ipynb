{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess tabular data\n",
    "\n",
    "We have 3 datasets about taxi fares. We have training, test and validation files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taxi-test.csv  taxi-train.csv taxi-valid.csv\n"
     ]
    }
   ],
   "source": [
    "! ls ../../datasets/Taxi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------> First row of tranining dataset:\n",
      "['11.3', '2011-01-28 20:42:59 UTC', '-73.999022', '40.739146', '-73.990369', '40.717866', '1', '0']\n"
     ]
    }
   ],
   "source": [
    "# Notice how this files doesn't have a tile for the columns.\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "try:\n",
    "    with open('../../datasets/Taxi/taxi-train.csv') as file:\n",
    "        csv_file = csv.reader(file)\n",
    "        print(\"------> First row of tranining dataset:\")\n",
    "        print(next(csv_file))\n",
    "except:\n",
    "    print(\"not able to open csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to be using the ```make_csv_dataset function``` , so we will need a couple of parameters.\n",
    "- A File Pattern-> This is a string that express the pattern in which the dataset's name was defined.\n",
    "- Name of the columns: Optional parameter that specifies the names of the columns on the csv data\n",
    "- batch size -> The number of values each batch will contain\n",
    "- Defaults-> The csv will have NaN values sometimes, when this happens we need to define some default values so the function can substitue\n",
    "- Target Column-> name of our target label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#File Pattern\n",
    "file_pattern = '../../datasets/Taxi/taxi-*.csv'\n",
    "CSV_COLUMNS = [\n",
    "    'fare_amount',\n",
    "    'pickup_datetime',\n",
    "    'pickup_longitude',\n",
    "    'pickup_latitude',\n",
    "    'dropoff_longitude',\n",
    "    'dropoff_latitude',\n",
    "    'passenger_count',\n",
    "    'key'\n",
    "]\n",
    "TARGET_LABEL = 'fare_amount'\n",
    "# Defining the default values into a list `DEFAULTS`\n",
    "DEFAULTS = [[0.0], ['na'], [0.0], [0.0], [0.0], [0.0], [0.0], ['na']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this parameters defined lets define a function that will return a dataset and lets examine the shape and type of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 5.33 GB\n",
      "\n",
      "--> dataset type: <class 'tensorflow.python.data.ops.dataset_ops.PrefetchDataset'>\n",
      "--> Type of item dataset: <class 'collections.OrderedDict'>\n",
      "    length:8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-02 12:17:39.652363: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2021-12-02 12:17:39.652522: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2021-12-02 12:17:39.692925: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2021-12-02 12:17:39.693176: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    }
   ],
   "source": [
    "def build_dataset(file_pattern, batch_size, column_names, column_defaults, label_name = None):\n",
    "    dataset = tf.data.experimental.make_csv_dataset(file_pattern, batch_size, column_names, column_defaults, label_name)\n",
    "    return dataset \n",
    "\n",
    "dataset = build_dataset(file_pattern, 1, CSV_COLUMNS, DEFAULTS)\n",
    "\n",
    "# Examine the newly created dataset:\n",
    "# Type:\n",
    "print(f'--> dataset type: {type(dataset)}')\n",
    "\n",
    "# Now Examine the internal items of the dataset\n",
    "# YOU WILL FIND THAT IT IS A DICTIONARY\n",
    "first_dataset_dictionary =next(iter(dataset))\n",
    "print(f'--> Type of item dataset: {type(first_dataset_dictionary)}')\n",
    "print(f'    length:{len(first_dataset_dictionary)}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key:fare_amount  %20s-> value:[7.]\n",
      "key:pickup_datetime  %20s-> value:[b'2013-12-09 15:03:00 UTC']\n",
      "key:pickup_longitude  %20s-> value:[-73.96412]\n",
      "key:pickup_latitude  %20s-> value:[40.76221]\n",
      "key:dropoff_longitude  %20s-> value:[-73.970695]\n",
      "key:dropoff_latitude  %20s-> value:[40.764526]\n",
      "key:passenger_count  %20s-> value:[1.]\n",
      "key:key  %20s-> value:[b'5473']\n"
     ]
    }
   ],
   "source": [
    "# Examine the values of the dictionary\n",
    "for key, value in iter(first_dataset_dictionary.items()):\n",
    "    print(f'key:{key}  %20s-> value:{value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: \n",
      "key:pickup_datetime    value:[b'2012-07-05 14:18:00 UTC']\n",
      "key:pickup_longitude    value:[-73.98929]\n",
      "key:pickup_latitude    value:[40.748703]\n",
      "key:dropoff_longitude    value:[-73.98122]\n",
      "key:dropoff_latitude    value:[40.755363]\n",
      "key:passenger_count    value:[1.]\n",
      "key:key    value:[b'1002']\n",
      "Label: \n",
      "tf.Tensor([4.9], shape=(1,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# now lets create a new dataset with features and target labels divided\n",
    "dataset_feature_label = build_dataset(file_pattern, 1, CSV_COLUMNS, DEFAULTS, TARGET_LABEL)\n",
    "\n",
    "\n",
    "def print_features_and_label(dataset):\n",
    "    features, label = next(iter(dataset))\n",
    "    print(\"Features: \")\n",
    "    for (key, value) in features.items():\n",
    "        print(f'key:{key}    value:{value}')\n",
    "    print(\"Label: \")\n",
    "    print(label)\n",
    "\n",
    "print_features_and_label(dataset_feature_label)\n",
    "\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove unwated columns\n",
    "\n",
    "We will repeat what we did before but we will do it manually. And also we will remove some unwated columns.\n",
    "1. remove unwanted columns\n",
    "2. Separate target label in a new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('pickup_longitude', <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-73.982315], dtype=float32)>), ('pickup_latitude', <tf.Tensor: shape=(1,), dtype=float32, numpy=array([40.776146], dtype=float32)>), ('dropoff_longitude', <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-73.98469], dtype=float32)>), ('dropoff_latitude', <tf.Tensor: shape=(1,), dtype=float32, numpy=array([40.768337], dtype=float32)>), ('passenger_count', <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>)])\n",
      "tf.Tensor([4.], shape=(1,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "UNWANTED_COLS = ['pickup_datetime', 'key']\n",
    "raw_ds = build_dataset(file_pattern, 1, CSV_COLUMNS, DEFAULTS)\n",
    "\n",
    "# lets apply these to one \n",
    "def proccess_just_one(dataset):\n",
    "    for features in dataset.take(1):\n",
    "        label = features.pop(TARGET_LABEL)\n",
    "        for col in UNWANTED_COLS:\n",
    "            features.pop(col)\n",
    "    return (features, label)\n",
    "\n",
    "\n",
    "features, label = proccess_just_one(raw_ds)\n",
    "print(features)\n",
    "print(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----OK---\n",
      "OrderedDict([('pickup_longitude', <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-73.94956], dtype=float32)>), ('pickup_latitude', <tf.Tensor: shape=(1,), dtype=float32, numpy=array([40.781025], dtype=float32)>), ('dropoff_longitude', <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-73.953926], dtype=float32)>), ('dropoff_latitude', <tf.Tensor: shape=(1,), dtype=float32, numpy=array([40.779068], dtype=float32)>), ('passenger_count', <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>)])\n",
      "(1,)\n"
     ]
    }
   ],
   "source": [
    "# lets create a map function\n",
    "def map_features_label(features, target_label,unwanted_columns):\n",
    "    label = features.pop(target_label)\n",
    "    for col in unwanted_columns:\n",
    "        features.pop(col)    \n",
    "    return (features, label)\n",
    "\n",
    "raw_ds = build_dataset(file_pattern, 1, CSV_COLUMNS, DEFAULTS)\n",
    "#features_labels_ds = raw_ds.map(lambda x : map_features_label(x, target_label=TARGET_LABEL, unwanted_columns=UNWANTED_COLS))\n",
    "features_labels_ds = raw_ds.map(lambda features : map_features_label(features, target_label=TARGET_LABEL, unwanted_columns=UNWANTED_COLS))\n",
    "\n",
    "# check everything went well\n",
    "for ds in features_labels_ds.take(1):\n",
    "    features, label = ds\n",
    "\n",
    "    assert UNWANTED_COLS[0] not in features.keys()\n",
    "    assert UNWANTED_COLS[1] not in features.keys()\n",
    "    assert label.shape == [1]    \n",
    "    print(\"----OK---\")\n",
    "    print(features)\n",
    "    print(label.shape)\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch and shuffle\n",
    "\n",
    "Finally lets create a function that receives batch size as parameters, divides the dataset into features and columns and finally shuffle the data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4bd624a0593993fe43ac4046b27b898fb2ef75c21c08f81e89e64ea0f51df676"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('tensorflow': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
