{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Datasets and CSV files\n",
    "\n",
    "### The linear regression with ```dataset```\n",
    "This is a reminder of the contents of the coursera Introduction to tensorflow, week two laboratory. It is important to remind yourself about the versatility of the data API. Begin with training the usage of the Dataset type. At the beggining you started using low level TF to obtain the weights of a line equation.\n",
    "\n",
    "$y = w0x + w1$\n",
    "\n",
    "You are going to repeat the process but with a small difference, you are going to incorporate the ```dataset``` type to the training loop. Here are the instructions:\n",
    "\n",
    "1. Create some X, Y vectors with data\n",
    "2. Create some **Datasets** from these tensors with a batch size of 3, this is a shape (3,)\n",
    "3. Verify this goes well and print it. It should have the following shape:\n",
    "\n",
    "```\n",
    "x: [0. 1. 2.] y: [10. 12. 14.]\n",
    "x: [3. 4. 5.] y: [16. 18. 20.]\n",
    "x: [6. 7. 8.] y: [22. 24. 26.]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0. 1. 2. 3. 4. 5. 6. 7. 8. 9.], shape=(10,), dtype=float32) tf.Tensor([ 2.  6. 10. 14. 18. 22. 26. 30. 34. 38.], shape=(10,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#begin importing tensorflow\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "# Create some tensors of shape (N_DATAPOINTS)\n",
    "N_DATAPOINTS = 10 \n",
    "# Define constants of the line m and b (w0, w1)\n",
    "m = 4\n",
    "b = 2\n",
    "\n",
    "# These are tensor types\n",
    "X = tf.constant(range(N_DATAPOINTS), dtype = tf.float32)\n",
    "Y = m*X + b \n",
    "print(X, Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xy_ds is a type TensorSliceDataset:<class 'tensorflow.python.data.ops.dataset_ops.TensorSliceDataset'>\n",
      "Each element of the dataset is a tuple (X, Y)\n",
      "<class 'tuple'>\n",
      "feature: 0.0 label: 2.0\n",
      "feature: 1.0 label: 6.0\n",
      "feature: 2.0 label: 10.0\n",
      "feature: 3.0 label: 14.0\n",
      "feature: 4.0 label: 18.0\n",
      "feature: 5.0 label: 22.0\n",
      "feature: 6.0 label: 26.0\n",
      "feature: 7.0 label: 30.0\n",
      "feature: 8.0 label: 34.0\n",
      "feature: 9.0 label: 38.0\n"
     ]
    }
   ],
   "source": [
    "# Now convert these into datasets\n",
    "xy_ds = tf.data.Dataset.from_tensor_slices((X,Y))\n",
    "# See what shape this dataset have\n",
    "print(f'xy_ds is a type TensorSliceDataset:{type(xy_ds)}')\n",
    "\n",
    "# Each item of this is a tuple\n",
    "print(f'Each element of the dataset is a tuple (X, Y)')\n",
    "for item in xy_ds.take(1):\n",
    "    print(type(item))\n",
    "\n",
    "# Lets visualize it\n",
    "for (feature, label) in xy_ds:\n",
    "    print(f'feature: {feature} label: {label}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature: [0. 1. 2.] label: [ 2.  6. 10.]\n",
      "feature: [3. 4. 5.] label: [14. 18. 22.]\n",
      "feature: [6. 7. 8.] label: [26. 30. 34.]\n",
      "feature: [9. 0. 1.] label: [38.  2.  6.]\n",
      "feature: [2. 3. 4.] label: [10. 14. 18.]\n",
      "feature: [5. 6. 7.] label: [22. 26. 30.]\n",
      "feature: [8. 9.] label: [34. 38.]\n"
     ]
    }
   ],
   "source": [
    "# Order the dataset with batch size = 3\n",
    "xy_ds = xy_ds.repeat(2).batch(3)\n",
    "for (feature, label) in xy_ds:\n",
    "    print(f'feature: {feature} label: {label}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:[0. 1. 2.] y:[ 2.  6. 10.]\n",
      "x:[3. 4. 5.] y:[14. 18. 22.]\n",
      "x:[6. 7. 8.] y:[26. 30. 34.]\n",
      "x:[9. 0. 1.] y:[38.  2.  6.]\n",
      "x:[2. 3. 4.] y:[10. 14. 18.]\n",
      "x:[5. 6. 7.] y:[22. 26. 30.]\n"
     ]
    }
   ],
   "source": [
    "# Lets embed this into a function\n",
    "def create_dataset(X,Y, epochs, batch_size):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((X,Y)).repeat(epochs).batch(batch_size, drop_remainder=True)\n",
    "    return dataset\n",
    "\n",
    "EPOCHS = 2\n",
    "BATCH_SIZE=3\n",
    "dataset = create_dataset(X,Y, EPOCHS, BATCH_SIZE)\n",
    "for (x,y) in dataset:\n",
    "    print(f'x:{x} y:{y}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now lets define the models for: Loss  and Gradient Computation\n",
    "$MSE = \\frac{1}{n}\\sum_{0}^{n}{(\\bar{y}-y})^2$\n",
    "\n",
    "This is the Mean Square Error, Where $\\bar{y}$ is our prediction and $y$ is the real value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Square error\n",
    "def mse(X,Y,w0,w1):\n",
    "    y_hat =  X*w0 + w1 \n",
    "    loss = (y_hat-Y)**2\n",
    "    mse = tf.reduce_mean(loss)\n",
    "    return mse\n",
    "\n",
    "# Compute Gradient\n",
    "def compute_gradients(X,Y,w0,w1):    \n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = mse(X,Y,w0,w1)\n",
    "        gradient = tape.gradient(loss, [w0,w1])\n",
    "        return gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 0, loss:18.051998138427734, w0:<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.11999999>, w1:<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.15999998>\n",
      "STEP 100, loss:0.05661267787218094, w0:<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=4.0432763>, w1:<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.7414136>\n",
      "STEP 200, loss:0.008423236198723316, w0:<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=4.016694>, w1:<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.9002551>\n",
      "STEP 300, loss:0.001253330148756504, w0:<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=4.0064397>, w1:<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.9615245>\n",
      "STEP 400, loss:0.00018649132107384503, w0:<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=4.002483>, w1:<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.9851589>\n",
      "STEP 500, loss:2.775211032712832e-05, w0:<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=4.0009575>, w1:<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.994275>\n",
      "STEP 600, loss:4.133353741053725e-06, w0:<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=4.000368>, w1:<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.9977912>\n",
      "STEP 700, loss:6.140874120319495e-07, w0:<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=4.0001435>, w1:<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.9991479>\n",
      "STEP 800, loss:9.177974646945586e-08, w0:<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=4.000056>, w1:<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.9996704>\n",
      "STEP 900, loss:1.3930845810250503e-08, w0:<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=4.0000215>, w1:<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.9998717>\n",
      "STEP 1000, loss:2.02590766384958e-09, w0:<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=4.0000095>, w1:<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.9999505>\n",
      "STEP 1100, loss:3.0059518763003723e-10, w0:<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=4.000004>, w1:<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.9999808>\n",
      "STEP 1200, loss:7.389652778577727e-11, w0:<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=4.000001>, w1:<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.9999909>\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 250\n",
    "BATCH_SIZE = 2\n",
    "LEARNING_RATE = 0.02\n",
    "MSG = \"STEP {step}, loss:{loss}, w0:{w0}, w1:{w1}\"\n",
    "#Now create the dataset \n",
    "training_ds  = create_dataset(X,Y, EPOCHS,BATCH_SIZE)\n",
    "# Initialize weights\n",
    "w0 = tf.Variable(0, dtype=tf.float32)\n",
    "w1 = tf.Variable(0, dtype=tf.float32)\n",
    "# Now iterate through the dataset\n",
    "for step, (x,y) in enumerate(training_ds):\n",
    "    dw0, dw1 = compute_gradients(x,y,w0,w1)\n",
    "    w0.assign_sub(dw0*LEARNING_RATE)\n",
    "    w1.assign_sub(dw1*LEARNING_RATE)\n",
    "\n",
    "    if step % 100 == 0:\n",
    "        loss = mse(x,y, w0, w1)\n",
    "        print(MSG.format(step=step, loss=loss, w0=w0, w1=w1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w0 = <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=4.000002> m=4\n",
      "w1 = <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.999994> b=2\n"
     ]
    }
   ],
   "source": [
    "# Compare\n",
    "print(f'w0 = {w0} m={m}')\n",
    "print(f'w1 = {w1} b={b}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with CSV files and Tensorflow Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['11.3', '2011-01-28 20:42:59 UTC', '-73.999022', '40.739146', '-73.990369', '40.717866', '1', '0']\n",
      "['5.3', '2012-01-03 19:21:35 UTC', '-73.962627', '40.763214', '-73.973485', '40.753353', '1', '0']\n",
      "['6.0', '2013-03-27 03:35:00 UTC', '-73.977672', '40.784052', '-73.965332', '40.801025', '2', '0']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "filepath_train = \"../../datasets/Taxi/taxi-train.csv\"\n",
    "filepath_valid = \"../../datasets/Taxi/taxi-valid.csv\"\n",
    "filepath_test = \"../../datasets/Taxi/taxi-test.csv\"\n",
    "\n",
    "def print_first_line_of_csv(filepath):\n",
    "    try:\n",
    "        with open(filepath) as file:\n",
    "            csv_file = csv.reader(file)\n",
    "            print(next(iter(csv_file)))\n",
    "    except:\n",
    "        print(\"Not able to open csv\")\n",
    "\n",
    "# See how this CSV files do not posses Column titles\n",
    "print_first_line_of_csv(filepath_train)\n",
    "print_first_line_of_csv(filepath_valid)\n",
    "print_first_line_of_csv(filepath_test)\n",
    "\n",
    "#lets define the Column titles and default values:\n",
    "# Defining the feature names into a list `CSV_COLUMNS`\n",
    "CSV_COLUMNS = [\n",
    "    'fare_amount',\n",
    "    'pickup_datetime',\n",
    "    'pickup_longitude',\n",
    "    'pickup_latitude',\n",
    "    'dropoff_longitude',\n",
    "    'dropoff_latitude',\n",
    "    'passenger_count',\n",
    "    'key'\n",
    "]\n",
    "LABEL_COLUMN = 'fare_amount'\n",
    "# Defining the default values into a list `DEFAULTS`\n",
    "DEFAULTS = [[0.0], ['na'], [0.0], [0.0], [0.0], [0.0], [0.0], ['na']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can proceed to act on this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type<class 'tensorflow.python.data.ops.dataset_ops.PrefetchDataset'>\n",
      "---------------dataset is made of a OrderedDict\n",
      "OrderedDict([('fare_amount', <tf.Tensor: shape=(1,), dtype=float32, numpy=array([6.9], dtype=float32)>), ('pickup_datetime', <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'2011-03-19 03:32:00 UTC'], dtype=object)>), ('pickup_longitude', <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-74.00206], dtype=float32)>), ('pickup_latitude', <tf.Tensor: shape=(1,), dtype=float32, numpy=array([40.73046], dtype=float32)>), ('dropoff_longitude', <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-73.98335], dtype=float32)>), ('dropoff_latitude', <tf.Tensor: shape=(1,), dtype=float32, numpy=array([40.756454], dtype=float32)>), ('passenger_count', <tf.Tensor: shape=(1,), dtype=float32, numpy=array([2.], dtype=float32)>), ('key', <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'2078'], dtype=object)>)])\n",
      "---------------Using numpy make it more readable\n",
      "{'fare_amount': array([7.3], dtype=float32), 'pickup_datetime': array([b'2010-01-16 15:51:09 UTC'], dtype=object), 'pickup_longitude': array([-73.97204], dtype=float32), 'pickup_latitude': array([40.759293], dtype=float32), 'dropoff_longitude': array([-73.9889], dtype=float32), 'dropoff_latitude': array([40.736645], dtype=float32), 'passenger_count': array([1.], dtype=float32), 'key': array([b'1235'], dtype=object)}\n"
     ]
    }
   ],
   "source": [
    "def create_dataset_from_csv(pattern):\n",
    "    return tf.data.experimental.make_csv_dataset(pattern, batch_size=1, column_names=CSV_COLUMNS, column_defaults=DEFAULTS)\n",
    "\n",
    "taxi_csv_ds = create_dataset_from_csv(\"../../datasets/Taxi/taxi-train.csv\")\n",
    "print(f'type{type(taxi_csv_ds)}')\n",
    "\n",
    "print(\"---------------dataset is made of a OrderedDict\")\n",
    "# print each dataset item\n",
    "for item in taxi_csv_ds.take(1):\n",
    "    print(item)\n",
    "\n",
    "# make it more readable using numpy\n",
    "print(\"---------------Using numpy make it more readable\")\n",
    "for data in taxi_csv_ds.take(1):\n",
    "    dict = { k:v.numpy() for (k,v) in data.items() }\n",
    "    print(dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4bd624a0593993fe43ac4046b27b898fb2ef75c21c08f81e89e64ea0f51df676"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('tensorflow': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
